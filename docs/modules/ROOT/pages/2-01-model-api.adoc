= 2.1 Serving a model

include::_attributes.adoc[]

In the previous section, we learned how to create the code that will use an existing object detection model to identify object in a static image. But of course we cannot use a notebook directly like this in a production environment.

So now we will learn how to service this model as an *API* that you can directly query from other applications. For this, we will use the model serving capabilities of RHODS:

* Return to the RHODS Dashboard and enter your Project. Let's now set up the model server by selecting *Configure server* in the model serving section and choosing these settings:

** Leave *Model server replicas* at 1.

** Under *Model server size* choose `Small`.

** Select *Make deployed models available through an external route*.

** Leave *Token authorization* unselected.

** Select *Configure* to finish model server setup.

* You're now ready to deploy the object detection model. Under model server select *Deploy model* and set it up as follows:

** Choose an arbitrary *Model name* such as `yolo-v3`.

** Under *Model framework* select `ONNX`.

** Under *Data connection* select the data connection that you have provisioned earlier.

** Under *Folder path* enter `tiny-yolov3-11.onnx`.

* Select *Deploy* to instantiate the model server. Click on `1` below _Deployed models_ to monitor the status of the model deployment. It will indicate a green status once the model can be consumed. Copy the _Inference endpoint_ so we can test the model deployment.

* In your workbench, open the `2_online-scoring` notebook. Insert the inference endpoint into the `prediction_url` placeholder and execute the notebook. You should see bounding boxes of the objects that the deployed model detected based on the image that it received.

You just deployed your first model with RHODS model serving. Let's now xref:2-02-deploy-s2i.adoc[deploy a small application that consumer the model].
